{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVrvipmUSSt",
        "outputId": "c7998828-2b51-45bf-d0ce-e417c3d2da9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=1...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=2...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=3...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=4...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=5...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=6...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=7...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=8...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=9...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=10...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=11...\n",
            "üîé Procesando https://www.huila.gov.co/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag=12...\n",
            "‚úÖ Listo! Se guardaron 120 noticias en noticias_huila.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://www.huila.gov.co\"\n",
        "LISTADO_URL = BASE_URL + \"/cultura-y-turismo/publicaciones/noticias/?tema=15&genPag={}\"\n",
        "\n",
        "# Cabeceras para evitar bloqueos\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "# Archivo CSV\n",
        "OUTPUT_FILE = \"noticias_huila.csv\"\n",
        "\n",
        "def extraer_detalle(url):\n",
        "    \"\"\"Extrae el contenido completo de una noticia desde su p√°gina interna.\"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        # Contenido de la noticia\n",
        "        contenedor = soup.select_one(\"div.publication-container .pgel\")\n",
        "        if contenedor:\n",
        "            parrafos = contenedor.find_all(\"p\")\n",
        "            contenido = \" \".join([p.get_text(\" \", strip=True) for p in parrafos])\n",
        "        else:\n",
        "            contenido = None\n",
        "\n",
        "        return contenido\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extraer_listado():\n",
        "    noticias = []\n",
        "\n",
        "    for pagina in range(1, 13):  # p√°ginas 1 a 12\n",
        "        url = LISTADO_URL.format(pagina)\n",
        "        print(f\"üîé Procesando {url}...\")\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "            # Iterar sobre cada noticia\n",
        "            for h2 in soup.select(\"div.modContent h2\"):\n",
        "                link_tag = h2.find(\"a\")\n",
        "                titulo = link_tag.get_text(strip=True) if link_tag else None\n",
        "                enlace = link_tag[\"href\"] if link_tag else None\n",
        "                if enlace and not enlace.startswith(\"http\"):\n",
        "                    enlace = BASE_URL + enlace\n",
        "\n",
        "                # Imagen y descripci√≥n (est√°n en el siguiente div.row)\n",
        "                row = h2.find_next_sibling(\"div\", class_=\"row\")\n",
        "                if row:\n",
        "                    img_tag = row.find(\"img\")\n",
        "                    imagen = BASE_URL + img_tag[\"src\"] if img_tag else None\n",
        "                    desc_tag = row.find(\"p\", class_=\"pgel desc-cat-doc\")\n",
        "                    descripcion = desc_tag.get_text(strip=True) if desc_tag else None\n",
        "                else:\n",
        "                    imagen, descripcion = None, None\n",
        "\n",
        "                # Fecha (est√° despu√©s del row)\n",
        "                fecha_tag = row.find_next_sibling(\"div\", class_=\"pubTema text-right\") if row else None\n",
        "                fecha = fecha_tag.get_text(strip=True) if fecha_tag else None\n",
        "\n",
        "                # Extraer contenido interno\n",
        "                contenido = extraer_detalle(enlace) if enlace else None\n",
        "\n",
        "                noticias.append({\n",
        "                    \"titulo\": titulo,\n",
        "                    \"enlace\": enlace,\n",
        "                    \"imagen\": imagen,\n",
        "                    \"descripcion\": descripcion,\n",
        "                    \"fecha\": fecha,\n",
        "                    \"contenido\": contenido\n",
        "                })\n",
        "\n",
        "                time.sleep(1)  # respetar al servidor\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error en la p√°gina {pagina}: {e}\")\n",
        "\n",
        "    return noticias\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    datos = extraer_listado()\n",
        "\n",
        "    # Guardar en CSV\n",
        "    with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"titulo\", \"enlace\", \"imagen\", \"descripcion\", \"fecha\", \"contenido\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(datos)\n",
        "\n",
        "    print(f\"‚úÖ Listo! Se guardaron {len(datos)} noticias en {OUTPUT_FILE}\")\n"
      ]
    }
  ]
}